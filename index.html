<!DOCTYPE html>
<!-- copy from raymin0223.github.io/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>June-Woo Kim</title>
<meta content="June-Woo Kim" name="June-Woo Kim">
<link href="./June-Woo_Kim_files/style.css" rel="stylesheet" type="text/css">
<script src="./June-Woo_Kim_files/jquery-1.11.1.min.js" type="text/javascript"></script>  
</head>


<body>
  <div class="menu"> <a href="https://kaen2891.github.io/profile/index.html">Home</a>  <a href="https://kaen2891.github.io/profile/index.html#publications">Publications</a>  
    <a href="https://kaen2891.github.io/profile/index.html#projects">Projects</a> <a href="https://kaen2891.github.io/profile/index.html#services"> Services</a>   <a href="https://kaen2891.github.io/profile/index.html#awards"> Awards</a> 
    <a href="https://kaen2891.github.io/profile/index.html#patents"> Patents</a> 
  </div>
  <div class="container">
    <table border="0">
      <tbody><tr>
        <td><img src="./June-Woo_Kim_files/bio-kaen.jpg" width="130"></td>
        <td style="width: 10px">&nbsp;</td>
        <td valign="top" width="500">
          <span class="name">June-Woo Kim</span>
          <p class="information"><br>
           Ph.D. Candidate, <a href="https://sites.google.com/view/mlc-lab">MLC Lab</a> </p>
          <p class="information">Department of Artificial Intelligence, Kyungpook National University<br>
            80, Daehak-ro, Buk-gu, Daegu, 41566, Korea<br></p>
          <p class="information"><strong>Email</strong>: <span class="unselectable">kaen2891<span class="mock"></span><span class="hide">xkxkxk</span>@knu.ac.kr</span> <span class="unselectable">/ kaen2891<span class="mock"></span><span class="hide">xkxkxk</span>@gmail.com</span> </br>
	  <a href="https://drive.google.com/drive/folders/1N5m6rgKG9F5y_2fslPoSKo0R9ZkAJ-pT?usp=drive_link">CV</a>, <a href="http://www.linkedin.com/in/june-woo-kim-043374204/">Linkedin</a>, <a href="https://github.com/kaen2891/">Github</a> </p>
        </td>
      </tr>
    </tbody></table>
  <strong>Welcome to my page!</strong> I am currently pursuing a Ph.D. at Kyungpook National University. While my main area of concentration has been in Speech Recognition, I have also delved into various other fields within AI such as NLP, Audio, Audio in Medical AI, and Video, with the aim of expanding my understanding and skills.    
   Specifically, I am interested in creating an <strong>ASR system that ensures fair speech recognition performance</strong> regardless of the speaker's personal characteristics, as well as <strong>medical AI including respiratory sound classification, speech-based psychiatry analysis and depression detection</strong>.

   <a id="news" class="anchor"></a><span class="section">News</span> 
    <p class="news">
     <strong>May. 2024:</strong> Starting a new position as Applied Scientist Intern at Amazon. I am now in UK!
    </p>
    
    <p class="news">
     <strong>Apr. 2024:</strong> I will attend ICASSP 2024 for my presentation. Let's get in touch in Seoul, Korea!.
    </p>
    
    <p class="news">
     <strong>Apr. 2024:</strong> A paper on 'Input-Agnostic Augmentation for Respiratory Sound Classification' accepted at EMBC 2024.
    </p>
    
    <p class="news">
     <strong>Jan. 2024:</strong> Starting a new position as Research Ph.D Internship at NAVER AI.
    </p>
    
    <p class="news">
     <strong>Dec. 2023:</strong> A paper on 'Cross-domain adaptation with Supervised Contrastive Learning on Respiratory Sound' accepted at ICASSP 2024.
    </p>
    

    <p>&nbsp;</p>

   <a id="education" class="anchor"></a><span class="section">Education</span> 

   <li style="line-height:160%;"> Ph.D. student in Department of Artificial Intelligence, Kyungpook National University. Advised by Prof. <a href="https://scholar.google.com/citations?user=gvaE8RUAAAAJ&hl=en"><font color="#000080">Ho-Young Jung</font></a>. <em>Present</em> </li> 
   <li style="line-height:160%;"> M.S. in Department of Artificial Intelligence, Kyungpook National University. Advised by Prof. <a href="https://scholar.google.com/citations?user=LOCg7vsAAAAJ&hl=en"><font color="#000080">Minho Lee</font></a>. <em>Feb. 2021</em> </li>
   <li style="line-height:160%;">  B.S. in Department of Information and Communication Convergence Engineering, Mokwon University. <em>Feb. 2017</em> </li>
   
   
   <a id="works" class="anchor"></a><span class="section">Work Experience</span> 
   
   <li style="line-height:160%;"> Applied Scientist Intern at Amazon; Improving Alexa shopping customers' ASR performance using synthetic speech based on TTS. Advised by Federica Cerina and Dhruv Agarwal. <em>May - Present</em> </li>

   <li style="line-height:160%;"> Research Ph.D internship at NAVER AI; Improving speech recognition performance in doctor-patient conversations utilizing speaker verification model, improving respiratory sound classification using prompted metadata as text description, psychiatry voice analysis. Advised by <a href="https://scholar.google.co.kr/citations?user=nHZWDlkAAAAJ&hl=en"><font color="#000080">Seong-Eun Moon</font></a>. <em>Jan - Apr 2024</em> </li>
	

   <p>&nbsp;</p>

    <!-- Publication session -->
    <a id="publications" class="anchor"></a><span class="section">Publications <a href="https://scholar.google.co.kr/citations?user=bMI8tY0AAAAJ&hl=en">  Google Scholar </a> </span>
    *: 1st co-authors, <sup>&dagger;</sup>: corresponding authors, C: conferences, J: journals, W: workshops, P: preprints </br> </br>
    
      <tbody><font size="4.5px"><strong>2024</strong></font>
	    <table border="0" width="90%" class="paper"><tbody>

        
        <tr>
          <td>
            <img src="./images/2024_bts.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
          
            [P1] <strong>J.-W. Kim</strong>, M. Toikkanen, Y. Choi, S.-E. Moon<sup>&dagger;</sup>, H.-Y. Jung<sup>&dagger;</sup>.  <strong>BTS: Bridging Text and Sound Modalities for Metadata-Aided Respiratory Sound Classification</strong>. <em> Preprint </em> .
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        <tr>
          <td>
            <img src="./images/2024_repaugment.png" class="PaperThumbnail" width="120" height="60">
          </td>
          
          <td bgcolor="#e9eaed">
            [C5] <strong>J.-W. Kim</strong>, M. Toikkanen, S. Bae, M. Kim<sup>&dagger;</sup>, H.-Y. Jung<sup>&dagger;</sup>.  <strong>RepAugment: Input-Agnostic Representation-Level Augmentation for Respiratory Sound Classification</strong>. <em> International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) </em> 2024.
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
        
        <tr>
          <td>
            <img src="./images/2023_sgscl.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
            [C4] <strong>J.-W. Kim</strong>, S. Bae, W.-Y. Cho, B. Lee, H.-Y. Jung<sup>&dagger;</sup>.  <strong>Stethoscope-guided Supervised Contrastive Learning for Cross-domain Adaptation on Respiratory Sound Classification</strong>. <em> IEEE International Conference on Acoustics, Speech and Signal Processing </em> (ICASSP) 2024.
            [<a href="https://arxiv.org/abs/2312.09603"><font color="#000080">pdf</font></a>]
            [<a href="https://github.com/kaen2891/stethoscope-guided_supervised_contrastive_learning"><font color="#000080">code</font></a>]
            <!-- [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>] -->
          </td>
        </tr>
        
      </tbody></table></br>
      
      <tbody><font size="4.5px"><strong>2023</strong></font>
	    <table border="0" width="90%" class="paper"><tbody>
        
        <tr>
          <td>
            <img src="./images/2023_aft.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td bgcolor="#e9eaed">
            [W1] <strong>J.-W. Kim</strong>, C. Yoon, M. Toikkanen, S. Bae, H.-Y. Jung<sup>&dagger;</sup>.  <strong>Adversarial Fine-tuning using Generated Respiratory Sound to Address Class Imbalance</strong>. <em>Neural Information Processing Systems Workshop on Deep Generative Models for Health</em> (NeurIPSW) 2023.
            [<a href="https://openreview.net/forum?id=z1AVG5LDQ7"><font color="#000080">webpage</font></a>]
            [<a href="https://anonymous.4open.science/r/Adversarial-Adaptation-Synthetic-Respiratory-Sound-Data-0547/README.md"><font color="#000080">demo</font></a>]
          </td>
        </tr>
        
        <tr>
          <td>
            <img src="./images/2023_spectral.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
           [J6] <strong>J.-W. Kim</strong>, H. Chung, H.-Y. Jung<sup>&dagger;</sup>.  <strong>Spectral Salt-and-Pepper Patch Masking for Self-Supervised Speech Representation Learning</strong>. <em>Mathematics</em> 2023.
            [<a href="https://doi.org/10.3390/math11153418"><font color="#000080">webpage</font></a>]
          </td>
        </tr>   

        <tr>
          <td>
            <img src="./images/2023_patchmix.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td bgcolor="#e9eaed">
           [C3] S. Bae*, <strong>J.-W. Kim*</strong>, W. Cho, H. Baek, S. Son, B. Lee, C. Ha, K. Tae, S. Kim<sup>&dagger;</sup>, S.-Y. Yun<sup>&dagger;</sup>.   <strong>Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on Respiratory Sound Classification</strong>. <em>Conference of the International Speech Communication Association </em>  (INTERSPEECH) 2023.
            [<a href="https://www.isca-archive.org/interspeech_2023/bae23b_interspeech.pdf"><font color="#000080">pdf</font></a>]
            [<a href="https://github.com/raymin0223/patch-mix_contrastive_learning"><font color="#000080">code</font></a>]
          </td>
        </tr>   

        <tr>
          <td>
            <img src="./images/2023_unsupervised.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
           [J5] <strong>J.-W. Kim</strong>, H. Chung, H.-Y. Jung<sup>&dagger;</sup>.   <strong>Unsupervised Representation Learning with Task-Agnostic Feature Masking for Robust End-to-End Speech Recognition </strong>. <em>Mathematics </em> 2023.
            [<a href="https://doi.org/10.3390/math11030622"><font color="#000080">webpage</font></a>]
          </td>
        </tr>
	</tbody></table></br>
  <font size="4.5px"><strong>2022</strong></font>
	<table border="0" width="90%" class="paper"><tbody>        

        <tr>
        <td>
            <img src="./images/2022_improved.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td bgcolor="#e9eaed">
           [J4] <strong>J.-W. Kim</strong>, H. Yoon, H.-Y. Jung<sup>&dagger;</sup>.  <strong>Improved Spoken Language Representation for Intent Understanding in a Task-Oriented Dialogue System</strong>. <em>Sensors</em> 2022. 
            [<a href="https://doi.org/10.3390/s22041509"><font color="#000080">webpage</font></a>]
          </td>
        </tr>    

	</tbody></table></br>
  <font size="4.5px"><strong>2021</strong></font>
	<table border="0" width="90%" class="paper"><tbody>
        <tr>
        <td>
            <img src="./images/2021_linguistic.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
           [J3] <strong>J.-W. Kim</strong>, H. Yoon, H.-Y. Jung<sup>&dagger;</sup>.  <strong>Linguistic-Coupled Age-to-Age Voice Translation to Improve Speech Recognition Performance in Real Environments</strong>. <em>IEEE ACCESS </em> 2021. <!--span class="oral">Oral Presentation.</span-->
            [<a href="https://doi.org/10.1109/ACCESS.2021.3115608"><font color="#000080">webpage</font></a>]
          </td>
        </tr>  
	  
	  
	</tbody></table></br>
	<font size="4.5px"><strong>2020</strong></font>
	<table border="0" width="90%" class="paper"><tbody>
	   
      <tr>
        <td>
          <img src="./images/2020_end.png" class="PaperThumbnail" width="120" height="60">
        </td>
        <td bgcolor="#e9eaed">
         [J2] <strong>J.-W. Kim</strong>, H.-Y. Jung<sup>&dagger;</sup>. <strong>End-to-End Speech Recognition Models using Limited Training Data</strong>. <em>Phonetics and Speech Sciences </em> 2020. 
		  [<a href="https://doi.org/10.13064/KSSS.2020.12.4.063"><font color="#000080">webpage</font></a>]
		  </td>
      </tr>

      <tr>
        <td>
          <img src="./images/2020_voice.png" class="PaperThumbnail" width="120" height="60">
        </td>
        <td>
         [J1] <strong>J.-W. Kim</strong>, H.-Y. Jung<sup>&dagger;</sup>.  <strong>Voice-to-voice Conversion using Transformer Network</strong>. <em>Phonetics and Speech Sciences </em> 2020. 
          [<a href="https://doi.org/10.13064/KSSS.2020.12.3.055"><font color="#000080">webpage</font></a>]
        </td>
      </tr>
      
      <tr>
        <td>
          <img src="./images/2020_vocoder.png" class="PaperThumbnail" width="120" height="60">
        </td>
        <td bgcolor="#e9eaed">
         [C2] <strong>J.-W. Kim</strong>, H.-Y. Jung M. Lee<sup>&dagger;</sup>. <strong>Vocoder-free End-to-End Voice Conversion with Transformer Network</strong>. <em>International Joint Conference on Neural Networks </em> (IJCNN) 2020. 
		  [<a href="https://arxiv.org/pdf/2002.03808.pdf"><font color="#000080">pdf</font></a>]
      [<a href="https://doi.org/10.1109/IJCNN48605.2020.9207653"><font color="#000080">webpage</font></a>]
		  [<a href="https://kaen2891.github.io/voice_conversion_ijcnn2020/"><font color="#000080">demo</font></a>]
		  </td>
      </tr>
        
      
	</tbody></table></br>
	<font size="4.5px"><strong>2018</strong></font>
	<table border="0" width="90%" class="paper"><tbody>
	  
        <tr>
        <td>
          <img src="./images/2018_end.png" class="PaperThumbnail" width="120" height="60">
        </td>
        <td>
         [C1] M. Chae, T-H. Kim, Y.H. Shin, <strong>J.-W. Kim</strong>, S.-Y. Lee<sup>&dagger;</sup>.  <strong>End-to-End Multimodal Emotion and Gender Recognition with Dynamic Weights of Joint Loss</strong>. <em>International Conference on Intelligent Robots and Systmes Workshop </em> (IROSW) 2018. 
          [<a href="https://arxiv.org/ftp/arxiv/papers/1809/1809.00758.pdf"><font color="#000080">pdf</font></a>]
          [<a href="https://github.com/MyungsuChae/IROS2018_ws"><font color="#000080">code</font></a>]
        </td>
      </tr>   
	  
    </tbody></table>

    <p>&nbsp;</p>

	 
    <a id="projects" class="anchor"></a><span class="section">Projects</span>

    
    <li style="line-height:160%;"> [<strong>ETRI</strong>] AI-based Broadcasting Media Editing for Content Analysis Simulator. <span class="oral">Project Manager</span>. <em>2023</em>  </li>     
    <!--
    <li style="line-height:160%;">  [<strong>NRF</strong>] Automatic Interpretation Technology Capable of Many-to-Many Multilingual Translation While Maintaining the Users' Voice Style. <span class="oral">Project Manager</span>. <em>2023-Present</em>   </li>
    -->
    <li style="line-height:160%;">  [<strong>ETRI</strong>] Unsupervised Speech Representation Learning for Robust Speech Recognition Performance. <span class="oral">Project Manager</span>. <em>2021-Present</em>  </li>     
    <li style="line-height:160%;"> [<strong>IITP</strong>] Innovative Prediction Intelligence Technology using Multimodal Information. <em>2021-Present</em>  </li>
    <li style="line-height:160%;"> [<strong>ADD</strong>] Context Awareness-based Automatic Report Generation. <em>2021-2023</em>  </li>

    <p>&nbsp;</p>

	 
    <a id="services" class="anchor"></a><span class="section">Services</span>
    <li style="line-height:160%;">  Research Collaboration with Seoul National University College of Medicine. <em>2023-Present</em> </li>
    <li style="line-height:160%;">  Research Collaboration with <a href="https://modulabs.co.kr/"><font color="#000080">MODULABS.</font></a> <em>2022-Present</em> </li>    
    <li style="line-height:160%;">  AI Researcher at KAIST AI (KI4AI), advised by Prof.<a href="https://scholar.google.com/citations?user=QXGini0AAAAJ&hl=en"><font color="#000080">Soo-Young LEE</font></a> <em>2017-2018</em> </li>
    
    <p>&nbsp;</p>

	 
    <a id="awards" class="anchor"></a><span class="section">Awards and Honors</span>
    <li style="line-height:160%;"> 4th place from Human Understanding AI Paper Contest in ETRI. <em>2023</em> </li>    
    <li style="line-height:160%;"> <strong>Grand Prize</strong> from KNU Graduate Student Paper Contest in KNU. <em>2022</em></li>
    <li style="line-height:160%;"> 7th place from Korean Speech Recognition AI Contest in Korea Ministry of Science and Technology Information and Communication. <em>2022</em></li>
    <li style="line-height:160%;"> 5th place from Human Understanding AI Paper Contest in ETRI. <em>2022</em></li>
    <li style="line-height:160%;"> <strong>Grand Prize</strong> from English Children Speech Recognition Hackathon Competition in National Information Society Agency (NIA). <em>2021</em></li>
    <li style="line-height:160%;"> <strong>Grand Prize</strong> from Korean Children Speech Recognition Hackathon Competition in National Information Society Agency (NIA). <em>2021</em></li>
    <li style="line-height:160%;"> First Prize (2th place) from ETRI AI Practice Tech Day 2021 in ETRI. <em>2021</em></li>
    <li style="line-height:160%;"> Bronze Prize (7th place) from the National Institute of Korean AI-Language Proficiency Assessment Contest. <em>2021</em> </li>
    <li style="line-height:160%;"> <strong>Grand Prize</strong> from ETRI AI Practice Tech Day 2020 in ETRI. <em>2020</em> </li>
    <li style="line-height:160%;"> <a href="https://kis.kaist.ac.kr/?document_srl=42541&mid=ki_events&sort_index=title&order_type=desc"><font color="#000080">Excellent Researcher Award</font></a> at KAIST Institute Awards in KAIST. <em>2018</em> </li>
    
    <p>&nbsp;</p>

	 
    <!--a id="patents" class="anchor"></a><span class="section">Patents</span>
    <li style="line-height:130%;"> Toward Enhanced Representation for Federated Re-Identification by Not-True Self Knowledge Distillation. S-Y. Yun, S. Kim, W. Chung, <strong>S. Bae</strong>. <em>Korea Patent Application</em>. </li>
    <li style="line-height:130%;"> Federated Learning System for Performing Individual Data Customized Federated Learning, Method for Federated Learning, and Client Aratus for Performing Same. J. Oh, S. Kim, S-Y. Yun, <strong>S. Bae</strong>, J. SHin, S. Kim, W. Chung. <em>US and Korea Patent Application</em>. </li>
    <li style="line-height:130%;"> System, Method, Computer-Readable Storage Medium and Computer Program for Federated Learning of Local Model based on Learning Direction of Global Model. G. Lee, M. Jeong, S-Y. Yun, <strong>S. Bae</strong>, J. Ahn, S. Kim, W. Chung. <em>US and Korea Patent Application</em>.     </li-->	
    
    </br>  

  <p><font color="#444444" face="Arial" size="2">&copy 2023 June-Woo Kim Thanks <a href="https://www.raymin0223.com/"><font color="#000080">Sangmin Bae</font></a> for the template. </font></p>

<!--
<div id="current_date">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.29.1/moment.min.js"></script>
    <script>
        const formattedDate = moment().format('YYYY-MM-DD');
        document.getElementById("current_date").innerHTML = `Last updated on: ${formattedDate}`;
    </script>
</div>
</-->

  </div>
  <script>
    var thumbnails = document.getElementsByClassName("PaperThumbnail");
    var i;
    for (i = 0; i < thumbnails.length; i++) {
      thumbnails[i].width = "120"
    }
  </script>  


</body></html>
